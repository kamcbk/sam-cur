{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIC and p-Hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Introduction](#intro)<br>\n",
    "1 - [The Akaike Information Criterion](#aic)<br>\n",
    "2 - [p-Values and p-Hacking](#p-hack)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will introduce the Akaike Information Criterion ($\\text{AIC}$) and give an example of p-Hacking. $\\text{AIC}$ is a way of assessing the quality of a model given a set of data, an important concept for checking model performance. The target audience of this notebook was such that there was no need for calculus based statistics, and hence, no direct calculations of liklihood functions. Instead, we focus on providing intuition and understanding of $\\text{AIC}$ rather then calculation. Our section in p-Values and p-Hacking involves a brief review of p-Values and provides an example of modifying your results, creating a bias known as p-Hacking. p-Hacking is very common in frequetist statistics today, and it's important to identify and be aware of the potential pitfalls that exists in data analysis.\n",
    "\n",
    "The data for this notebook come from a study looking at NO$_2$ levels in the United States ([Novotny et al ES&T, 2011](https://pubs.acs.org/doi/abs/10.1021/es103578x)). The US Environmental Protection Agency collected this data in 2006 (known as *allmodelbuildingdata*) and the authors of this paper selected a handful of features in order to give create a positively correlated regression model (known as *final_model*). We'll be using both datasets in this notebook.\n",
    "\n",
    "This notebook has been created as a small introduction to $\\text{AIC}$ and p-Hacking, bordering an interactie notebook and a homework assignment. This notebook was created in part for [ER-190C, a course taught in Fall 2018 at UC Berkeley](https://github.com/ds-modules/ER-190C). What is presented here is but a sample of the completed homework assignment for the class. All materials (explainations, problem, solutions, etc.), except the data, were created by me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Akaike Information Criterion<a id='aic'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Akaike Information Criterion ([$\\text{AIC}$](https://www.youtube.com/watch?v=Nco_kh8xJDs)) assess the ***quality*** of a model given a set of data. Depending on the data that we use in our model, in this case, the features we add, AIC may be used to tell us how our model performs with the data given. Sometimes adding more data (features) improves the quality, sometimes less. Other times adding the right features may improve the quality.\n",
    "\n",
    "$\\text{AIC}$ is important because we can use it as a form of model selection. **Our goal is to find a model that has the highest *quality* given a list of models.** The higher the quality, the better our model performs and the more desirable it is. Your job in this section is to add features to *final_model* from *allmodelbulidingdata* and assess whether adding specific features improves the model or not. This may seem daunting, but we'll guide you in this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have some information about what AIC is doing and why it's important, we define $\\text{AIC}$ as the following:\n",
    "\n",
    "$\\text{AIC} = 2 \\times (\\text{number of features}) - 2 \\times \\log(\\text{maximum value of likelihood function})$\n",
    "\n",
    "where $\\log$ is $\\ln$.  The smaller $\\text{AIC}$ is, the greater the model performs (lower test error). A likelihood function is a statistical topic that we won't go into, but we'll provide the code on how to implement it.\n",
    "\n",
    "The way to interpret an $\\text{AIC}$ number is that smaller it is, the greater the model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load and clean both datasets, *final_model* and *allmodelbuildingdata*, so that we may be able to experiment with implementing models with different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading allmodelbuildingdata, no cleaning necessary since \n",
    "NO2_all = pd.read_csv('BechleLUR_2006_allmodelbuildingdata.csv')\n",
    "\n",
    "\n",
    "#Loading final_model daya\n",
    "NO2 = pd.read_csv(\"BechleLUR_2006_finalmodel.csv\")\n",
    "\n",
    "#Cleaning data, removing categorical and non-predictive variables\n",
    "NO2.drop(columns = \n",
    "         [\"Latitude\", \"Longitude\", \"State\", \"Predicted_NO2_ppb\", \n",
    "            \"Location_type\", \"Monitor_ID\"], inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our dataset, now we need a function to produce our maximum value of our liklihood function. Let's import what we need for $\\text{AIC}$. However, before we do that we must add a column of zeros to our model as it's a multiple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3270.3083800895165"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add column of zeros to adjust for constants\n",
    "#This is needed in regression models\n",
    "NO2[\"Constant\"] = np.ones(len(NO2))\n",
    "\n",
    "#Import to find maximum value of log likelihood\n",
    "import statsmodels.regression.linear_model as sm\n",
    "\n",
    "    \n",
    "#Function to find log likelihood and number of features\n",
    "def likandfeatures(X, y):\n",
    "    '''\n",
    "    Description: A function created to find the maximum value of the log-likelihood function in an OLS\n",
    "    model. Also returns the number of features in the likelihood\n",
    "    \n",
    "    Inputs: X, An NxN pandas dataframe that contains your features\n",
    "            y, An Nx1 pandas dataframe that you're trying to fit with X\n",
    "            \n",
    "    Returns: A tuple containing:\n",
    "               The amount of features in X when finding the log-likelihood function\n",
    "               The maximum value of the log-likelihood function\n",
    "    '''\n",
    "    #Adding a column (feature) of ones to consider constants because\n",
    "    #Statsmodel does not support constants, so we must manually add one.\n",
    "    #Thus, features = number of columns in X + 1\n",
    "    X_final = X.assign(Constant = np.ones(len(y)))\n",
    "    \n",
    "    return (sm.OLS(y, X_final).fit().llf, len(X_final.columns))\n",
    "\n",
    "\n",
    "#Running Model. This finds the Log-Likelihood function\n",
    "log_L = sm.OLS(NO2_all[\"Population_800\"], NO2).fit().llf\n",
    "log_L\n",
    "#-3270.3083800895165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1** Defining the $\\text{AIC}$ function outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aic(features, log_lik):\n",
    "    '''\n",
    "    A function that calculates the Akaike Information Criterion given the number of\n",
    "    features and maximum value of our liklihood function.\n",
    "    \n",
    "    Args:\n",
    "        features: int, num of features\n",
    "        log_lik: float, maximum value of our log liklihood function\n",
    "    Returns:\n",
    "        Float, a solution to the formula. The lesser the number the greater the model.\n",
    "    '''\n",
    "       return 2*(features - log_lik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a function to get our $\\text{AIC}$, we now run several tests to see if it works. We use statsmodels to confirm that our $\\text{AIC}$ is correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2** Look at the table produced by statsmodels below. It should show a lot of information, including $\\text{AIC}$. How close is our homemade $\\text{AIC}$ with statsmodels? The fact that our $\\text{AIC}$ is negative is simply a byproduct of our formula and doesn't have any meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the log-likelihood max value 8\n",
      "This is the aic: -6556.616760179033\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>Population_800</td>  <th>  R-squared:         </th> <td>   0.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   41.62</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 Jan 2019</td> <th>  Prob (F-statistic):</th> <td>7.65e-43</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:03:40</td>     <th>  Log-Likelihood:    </th> <td> -3270.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   369</td>      <th>  AIC:               </th> <td>   6557.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   361</td>      <th>  BIC:               </th> <td>   6588.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Observed_NO2_ppb</th>       <td>   22.9083</td> <td>   29.185</td> <td>    0.785</td> <td> 0.433</td> <td>  -34.486</td> <td>   80.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WRF+DOMINO</th>             <td>  110.6330</td> <td>   29.365</td> <td>    3.768</td> <td> 0.000</td> <td>   52.885</td> <td>  168.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>800m_Impervious_%</th>      <td>   28.7800</td> <td>    5.853</td> <td>    4.917</td> <td> 0.000</td> <td>   17.269</td> <td>   40.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Elevation_truncated_km</th> <td>-2107.6038</td> <td> 1008.378</td> <td>   -2.090</td> <td> 0.037</td> <td>-4090.636</td> <td> -124.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>800m_MajorRoads_km</th>     <td>  179.3514</td> <td>   48.135</td> <td>    3.726</td> <td> 0.000</td> <td>   84.691</td> <td>  274.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100m_MinorRoads_km</th>     <td>  813.2976</td> <td>  594.456</td> <td>    1.368</td> <td> 0.172</td> <td> -355.734</td> <td> 1982.330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Distance_to_coast_km</th>   <td>    0.1480</td> <td>    0.220</td> <td>    0.674</td> <td> 0.501</td> <td>   -0.284</td> <td>    0.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Constant</th>               <td> -609.8006</td> <td>  244.238</td> <td>   -2.497</td> <td> 0.013</td> <td>-1090.108</td> <td> -129.493</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>484.991</td> <th>  Durbin-Watson:     </th> <td>   1.742</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>62363.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 6.209</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>65.466</td>  <th>  Cond. No.          </th> <td>7.73e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 7.73e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:         Population_800   R-squared:                       0.447\n",
       "Model:                            OLS   Adj. R-squared:                  0.436\n",
       "Method:                 Least Squares   F-statistic:                     41.62\n",
       "Date:                Wed, 09 Jan 2019   Prob (F-statistic):           7.65e-43\n",
       "Time:                        16:03:40   Log-Likelihood:                -3270.3\n",
       "No. Observations:                 369   AIC:                             6557.\n",
       "Df Residuals:                     361   BIC:                             6588.\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================\n",
       "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------\n",
       "Observed_NO2_ppb          22.9083     29.185      0.785      0.433     -34.486      80.302\n",
       "WRF+DOMINO               110.6330     29.365      3.768      0.000      52.885     168.381\n",
       "800m_Impervious_%         28.7800      5.853      4.917      0.000      17.269      40.291\n",
       "Elevation_truncated_km -2107.6038   1008.378     -2.090      0.037   -4090.636    -124.572\n",
       "800m_MajorRoads_km       179.3514     48.135      3.726      0.000      84.691     274.011\n",
       "100m_MinorRoads_km       813.2976    594.456      1.368      0.172    -355.734    1982.330\n",
       "Distance_to_coast_km       0.1480      0.220      0.674      0.501      -0.284       0.580\n",
       "Constant                -609.8006    244.238     -2.497      0.013   -1090.108    -129.493\n",
       "==============================================================================\n",
       "Omnibus:                      484.991   Durbin-Watson:                   1.742\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            62363.535\n",
       "Skew:                           6.209   Prob(JB):                         0.00\n",
       "Kurtosis:                      65.466   Cond. No.                     7.73e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 7.73e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Homemade implementation\n",
    "homeAIC = likandfeatures(NO2, NO2_all['Population_800'])\n",
    "print(\"This is the log-likelihood max value\", homeAIC[1])\n",
    "print(\"This is the aic:\", aic(*homeAIC))\n",
    "\n",
    "#Statsmodels implementation\n",
    "sm.OLS(NO2_all[\"Population_800\"], NO2.assign(Constant = np.ones(len(NO2_all)))).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3** Now look at this statsmodels table. How does it compare with our homemade model? Is it greater, or less than the previous feature used (`Population_800`). Why do you think the feature `Major_400` produces the results that it does? *Hint*: Look at the features in `NO2`. Could any of these features be colinear with `Population_800`? How about `Major_400`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the log-likelihood max value 8\n",
      "This is the aic: -1043.2092697974288\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Major_400</td>    <th>  R-squared:         </th> <td>   0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   36.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 Jan 2019</td> <th>  Prob (F-statistic):</th> <td>2.10e-38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:53:12</td>     <th>  Log-Likelihood:    </th> <td> -513.60</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   369</td>      <th>  AIC:               </th> <td>   1043.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   361</td>      <th>  BIC:               </th> <td>   1074.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Observed_NO2_ppb</th>       <td>    0.0541</td> <td>    0.017</td> <td>    3.253</td> <td> 0.001</td> <td>    0.021</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WRF+DOMINO</th>             <td>   -0.0417</td> <td>    0.017</td> <td>   -2.494</td> <td> 0.013</td> <td>   -0.075</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>800m_Impervious_%</th>      <td>    0.0041</td> <td>    0.003</td> <td>    1.221</td> <td> 0.223</td> <td>   -0.002</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Elevation_truncated_km</th> <td>   -1.3624</td> <td>    0.574</td> <td>   -2.372</td> <td> 0.018</td> <td>   -2.492</td> <td>   -0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>800m_MajorRoads_km</th>     <td>    0.3019</td> <td>    0.027</td> <td>   11.014</td> <td> 0.000</td> <td>    0.248</td> <td>    0.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100m_MinorRoads_km</th>     <td>   -0.9444</td> <td>    0.339</td> <td>   -2.790</td> <td> 0.006</td> <td>   -1.610</td> <td>   -0.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Distance_to_coast_km</th>   <td>    0.0002</td> <td>    0.000</td> <td>    1.520</td> <td> 0.129</td> <td>-5.58e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Constant</th>               <td>   -0.0720</td> <td>    0.139</td> <td>   -0.517</td> <td> 0.605</td> <td>   -0.345</td> <td>    0.202</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>436.659</td> <th>  Durbin-Watson:     </th> <td>   1.939</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>32220.926</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 5.357</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>47.507</td>  <th>  Cond. No.          </th> <td>7.73e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 7.73e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              Major_400   R-squared:                       0.414\n",
       "Model:                            OLS   Adj. R-squared:                  0.402\n",
       "Method:                 Least Squares   F-statistic:                     36.39\n",
       "Date:                Wed, 09 Jan 2019   Prob (F-statistic):           2.10e-38\n",
       "Time:                        15:53:12   Log-Likelihood:                -513.60\n",
       "No. Observations:                 369   AIC:                             1043.\n",
       "Df Residuals:                     361   BIC:                             1074.\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================\n",
       "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------\n",
       "Observed_NO2_ppb           0.0541      0.017      3.253      0.001       0.021       0.087\n",
       "WRF+DOMINO                -0.0417      0.017     -2.494      0.013      -0.075      -0.009\n",
       "800m_Impervious_%          0.0041      0.003      1.221      0.223      -0.002       0.011\n",
       "Elevation_truncated_km    -1.3624      0.574     -2.372      0.018      -2.492      -0.233\n",
       "800m_MajorRoads_km         0.3019      0.027     11.014      0.000       0.248       0.356\n",
       "100m_MinorRoads_km        -0.9444      0.339     -2.790      0.006      -1.610      -0.279\n",
       "Distance_to_coast_km       0.0002      0.000      1.520      0.129   -5.58e-05       0.000\n",
       "Constant                  -0.0720      0.139     -0.517      0.605      -0.345       0.202\n",
       "==============================================================================\n",
       "Omnibus:                      436.659   Durbin-Watson:                   1.939\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            32220.926\n",
       "Skew:                           5.357   Prob(JB):                         0.00\n",
       "Kurtosis:                      47.507   Cond. No.                     7.73e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 7.73e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Homemade implementation\n",
    "homeAIC = likandfeatures(NO2, NO2_all[\"Major_400\"])\n",
    "print(\"This is the log-likelihood max value\", homeAIC[1])\n",
    "print(\"This is the aic:\", aic(*homeAIC))\n",
    "\n",
    "#Statsmodels implementation\n",
    "sm.OLS(NO2_all[\"Major_400\"], NO2.assign(Constant = np.ones(len(NO2_all)))).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4** How does this compare to the past two features? Why do you think it produced the results that it did relative to the past two features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the log-likelihood max value 8\n",
      "This is the aic: -2094.8362071539236\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Major_1200</td>    <th>  R-squared:         </th> <td>   0.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.601</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   80.28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 Jan 2019</td> <th>  Prob (F-statistic):</th> <td>1.02e-69</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:54:27</td>     <th>  Log-Likelihood:    </th> <td> -1039.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   369</td>      <th>  AIC:               </th> <td>   2095.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   361</td>      <th>  BIC:               </th> <td>   2126.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Observed_NO2_ppb</th>       <td>    0.2352</td> <td>    0.069</td> <td>    3.403</td> <td> 0.001</td> <td>    0.099</td> <td>    0.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WRF+DOMINO</th>             <td>   -0.1135</td> <td>    0.070</td> <td>   -1.633</td> <td> 0.103</td> <td>   -0.250</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>800m_Impervious_%</th>      <td>    0.0657</td> <td>    0.014</td> <td>    4.741</td> <td> 0.000</td> <td>    0.038</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Elevation_truncated_km</th> <td>   -3.2981</td> <td>    2.388</td> <td>   -1.381</td> <td> 0.168</td> <td>   -7.994</td> <td>    1.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>800m_MajorRoads_km</th>     <td>    1.5682</td> <td>    0.114</td> <td>   13.759</td> <td> 0.000</td> <td>    1.344</td> <td>    1.792</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100m_MinorRoads_km</th>     <td>   -2.0478</td> <td>    1.408</td> <td>   -1.455</td> <td> 0.147</td> <td>   -4.816</td> <td>    0.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Distance_to_coast_km</th>   <td>   -0.0003</td> <td>    0.001</td> <td>   -0.502</td> <td> 0.616</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Constant</th>               <td>   -0.0858</td> <td>    0.578</td> <td>   -0.148</td> <td> 0.882</td> <td>   -1.223</td> <td>    1.052</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>153.059</td> <th>  Durbin-Watson:     </th> <td>   2.110</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 656.776</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.784</td>  <th>  Prob(JB):          </th> <td>2.42e-143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.476</td>  <th>  Cond. No.          </th> <td>7.73e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 7.73e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:             Major_1200   R-squared:                       0.609\n",
       "Model:                            OLS   Adj. R-squared:                  0.601\n",
       "Method:                 Least Squares   F-statistic:                     80.28\n",
       "Date:                Wed, 09 Jan 2019   Prob (F-statistic):           1.02e-69\n",
       "Time:                        15:54:27   Log-Likelihood:                -1039.4\n",
       "No. Observations:                 369   AIC:                             2095.\n",
       "Df Residuals:                     361   BIC:                             2126.\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================\n",
       "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------\n",
       "Observed_NO2_ppb           0.2352      0.069      3.403      0.001       0.099       0.371\n",
       "WRF+DOMINO                -0.1135      0.070     -1.633      0.103      -0.250       0.023\n",
       "800m_Impervious_%          0.0657      0.014      4.741      0.000       0.038       0.093\n",
       "Elevation_truncated_km    -3.2981      2.388     -1.381      0.168      -7.994       1.397\n",
       "800m_MajorRoads_km         1.5682      0.114     13.759      0.000       1.344       1.792\n",
       "100m_MinorRoads_km        -2.0478      1.408     -1.455      0.147      -4.816       0.720\n",
       "Distance_to_coast_km      -0.0003      0.001     -0.502      0.616      -0.001       0.001\n",
       "Constant                  -0.0858      0.578     -0.148      0.882      -1.223       1.052\n",
       "==============================================================================\n",
       "Omnibus:                      153.059   Durbin-Watson:                   2.110\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              656.776\n",
       "Skew:                           1.784   Prob(JB):                    2.42e-143\n",
       "Kurtosis:                       8.476   Cond. No.                     7.73e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 7.73e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Homemade implementation\n",
    "homeAIC = likandfeatures(NO2, NO2_all[\"Major_1200\"])\n",
    "print(\"This is the log-likelihood max value\", homeAIC[1])\n",
    "print(\"This is the aic:\", aic(*homeAIC))\n",
    "\n",
    "#Statsmodels implementation\n",
    "sm.OLS(NO2_all[\"Major_1200\"], NO2.assign(Constant = np.ones(len(NO2_all)))).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. p-Values and p-Hacking<a id='p-hack'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we'll load NO2 data and use the package Statsmodels to help us calculate p-Values. We'll do some analysis on our results and discuss a potential danger/caution when making conclusions based on p-Values: p-Hacking. We assume some familarity with p-Values, however, we've taken the liberty of reviewing it here (as from experience, understanding p-Values can be sometimes tricky)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A p-value (taken from the [Data 8 textbook](https://www.inferentialthinking.com/chapters/11/3/Decisions_and_Uncertainty)) is the chance, based on the model in the null hypothesis, that the test statistic is equal to the value that was observed in the data or is even further in the direction of the alternative. Informally, if a p-value is really low, then the chance of your observation happening is low and may not be probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create a multiple regression model and find it's p-values by using the package Statsmodels. We'll load data from the final data at a study looking at NO$_2$ levels in the United States ([Novotny et al ES&T, 2011](https://pubs.acs.org/doi/abs/10.1021/es103578x))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observed_NO2_ppb</th>\n",
       "      <th>WRF+DOMINO</th>\n",
       "      <th>800m_Impervious_%</th>\n",
       "      <th>Elevation_truncated_km</th>\n",
       "      <th>800m_MajorRoads_km</th>\n",
       "      <th>100m_MinorRoads_km</th>\n",
       "      <th>Distance_to_coast_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.884706</td>\n",
       "      <td>11.615223</td>\n",
       "      <td>58.9488</td>\n",
       "      <td>0.304</td>\n",
       "      <td>1.35858</td>\n",
       "      <td>0.61637</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.089886</td>\n",
       "      <td>11.472677</td>\n",
       "      <td>71.4093</td>\n",
       "      <td>0.304</td>\n",
       "      <td>1.55566</td>\n",
       "      <td>0.26126</td>\n",
       "      <td>323.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.281969</td>\n",
       "      <td>8.990372</td>\n",
       "      <td>53.5480</td>\n",
       "      <td>0.304</td>\n",
       "      <td>1.59508</td>\n",
       "      <td>0.39460</td>\n",
       "      <td>308.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Observed_NO2_ppb  WRF+DOMINO  800m_Impervious_%  Elevation_truncated_km  \\\n",
       "0         23.884706   11.615223            58.9488                   0.304   \n",
       "1         25.089886   11.472677            71.4093                   0.304   \n",
       "2         19.281969    8.990372            53.5480                   0.304   \n",
       "\n",
       "   800m_MajorRoads_km  100m_MinorRoads_km  Distance_to_coast_km  \n",
       "0             1.35858             0.61637                 313.0  \n",
       "1             1.55566             0.26126                 323.8  \n",
       "2             1.59508             0.39460                 308.4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading data\n",
    "NO2 = pd.read_csv(\"BechleLUR_2006_finalmodel.csv\")\n",
    "\n",
    "#Cleaning data, removing categorical and non-predictive variables\n",
    "NO2.drop(columns = \n",
    "         [\"Latitude\", \"Longitude\", \"State\", \"Predicted_NO2_ppb\", \n",
    "            \"Location_type\", \"Monitor_ID\"], inplace = True)\n",
    "\n",
    "NO2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use \"Observed_NO2_ppb\" as our response variable and the rest of the variables we'll label as predictors. Then we'll run a multiple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting variables \n",
    "X = NO2.drop(\"Observed_NO2_ppb\", axis=1)\n",
    "Y = NO2[\"Observed_NO2_ppb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       Observed_NO2_ppb   R-squared:                       0.778\n",
      "Model:                            OLS   Adj. R-squared:                  0.775\n",
      "Method:                 Least Squares   F-statistic:                     212.0\n",
      "Date:                Tue, 08 Jan 2019   Prob (F-statistic):          3.36e-115\n",
      "Time:                        15:10:09   Log-Likelihood:                -938.93\n",
      "No. Observations:                 369   AIC:                             1892.\n",
      "Df Residuals:                     362   BIC:                             1919.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                      2.5572      0.419      6.106      0.000       1.734       3.381\n",
      "WRF+DOMINO                 0.7204      0.037     19.514      0.000       0.648       0.793\n",
      "800m_Impervious_%          0.0943      0.009     10.134      0.000       0.076       0.113\n",
      "Elevation_truncated_km    10.6405      1.728      6.159      0.000       7.243      14.038\n",
      "800m_MajorRoads_km         0.3210      0.085      3.776      0.000       0.154       0.488\n",
      "100m_MinorRoads_km         2.4832      1.063      2.337      0.020       0.394       4.573\n",
      "Distance_to_coast_km      -0.0012      0.000     -3.055      0.002      -0.002      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                       24.616   Durbin-Watson:                   1.351\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               31.898\n",
      "Skew:                           0.533   Prob(JB):                     1.18e-07\n",
      "Kurtosis:                       3.968   Cond. No.                     7.37e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 7.37e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# In order to have an intercept, we need to add a column of 1's to X\n",
    "# This is needed in regression models\n",
    "X2 = sm.add_constant(X)\n",
    "\n",
    "#Running multiple regression\n",
    "sm_model = sm.OLS(Y, X2)\n",
    "results = sm_model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels gives access to a ton of information; it may be a little daunting seeing it for the first time. However we may easily extract the p-Values from our regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const                     2.634006e-09\n",
       "WRF+DOMINO                1.840009e-58\n",
       "800m_Impervious_%         2.056259e-21\n",
       "Elevation_truncated_km    1.950387e-09\n",
       "800m_MajorRoads_km        1.865722e-04\n",
       "100m_MinorRoads_km        1.998295e-02\n",
       "Distance_to_coast_km      2.417248e-03\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Statsmodels, the null hypothesis is defined as there being no statistically significant relationship between the term ($x$) and our prediction ($\\hat{y}$). Rejecting the null hypothesis is dependent on the $\\alpha$ level, the minimum percentage that you're willing to accept the null. The smaller your $\\alpha$, the stricter your test. You'll be more confident at determining whether what you're measuring is likely due to chance or bias.\n",
    "\n",
    "**Question 2.1:** Interpret the p-values for each of the seven variables in results. Determine whether there is a statistically significant relationship between each variable and your predicted variable. You are free to choose your own $\\alpha$ value, whatever you feel is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n",
    "<br> $\\alpha$ = ...\n",
    "<br> const: ...\n",
    "<br> WRF+DOMINO: ...\n",
    "<br> 800m\\_Impervious_%: ...\n",
    "<br> Elevation_truncated_km: ...\n",
    "<br> 800m_MajorRoads_km: ...\n",
    "<br> 100m_MinorRoads_km: ...\n",
    "<br> Distance_to_coast_km: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution 1***\n",
    "**<br> $\\alpha$ = 0.05\n",
    "<br> const: There is a statistically significant relationship between const and our prediction\n",
    "<br> WRF+DOMINO: There is a statistically significant relationship between WRF+DOMINO and our prediction\n",
    "<br> 800m\\_Impervious_%: There is a statistically significant relationship between 800m\\_Impervious_% and our prediction\n",
    "<br> Elevation_truncated_km: There is a statistically significant relationship between Elevation_truncated_km and our prediction\n",
    "<br> 800m_MajorRoads_km: There is a statistically significant relationship between 800m_MajorRoads_km and our prediction\n",
    "<br> 100m_MinorRoads_km: There is no relationship between 100m_MinorRoads_km and our prediction\n",
    "<br> Distance_to_coast_km: There is a statistically significant relationship between Distance_to_coast_km and our prediction**\n",
    "\n",
    "***Solution 2***\n",
    "**<br> $\\alpha$ = 0.01\n",
    "<br> const: There is a statistically significant relationship between const and our prediction\n",
    "<br> WRF+DOMINO: There is a statistically significant relationship between WRF+DOMINO and our prediction\n",
    "<br> 800m\\_Impervious_%: There is a statistically significant relationship between 800m\\_Impervious_% and our prediction\n",
    "<br> Elevation_truncated_km: There is a statistically significant relationship between Elevation_truncated_km and our prediction\n",
    "<br> 800m_MajorRoads_km: There is a statistically significant relationship between 800m_MajorRoads_km and our prediction\n",
    "<br> 100m_MinorRoads_km: There is a statistically significant relationship between 100m_MinorRoads_km and our prediction\n",
    "<br> Distance_to_coast_km: There is a statistically significant relationship between Distance_to_coast_km and our prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your $\\alpha$ level, some variables may be statistically significant. The bias associated with choosing a p-value to determine significance is called *p-hacking*. In this case, choosing a higher or lower $\\alpha$ level as a result of seeing the p-values is subject to this bias (in other words, unless you have a standard go-to $\\alpha$ level, you were p-hacking). It's often best practice to pick an $\\alpha$ level *before* seeing your results.\n",
    "\n",
    "In general, *p-hacking* is the act of analyzing data in order to find patterns that are statistically significant that in reality have no actual significance. One usually has a bias on whether to reject the null or not, often subconciouslly, but sometimes deliberate. It's easy to fall prey to this bias, and it's important to keep in mind that when doing these hypothesis testings. \n",
    "\n",
    "In creating `results`, we added an extra column of ones in order to fit our model properly. Let's dig a little deeper with `const`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2:** Comment further on the statistical significance with the extra column of ones and your prediction. What does a column of ones represents? Are the ones truely in your data? Why doesn't it make sense or matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution***\n",
    "\n",
    "**A column of ones doesn't represent any of our original data. There can't be anything statistically significant about it with relation to our analysis/inference about our data. A column of ones is mearly to aid in our model fitting. It's meaningless to try to give it meaning because the column of ones is fabricated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A humorous example of this taken to an outragous level can be found on [Tyler Vigen's website](http://www.tylervigen.com/spurious-correlations). Pushing for significance between your variables without understanding their context leaves you subject to p-hacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
